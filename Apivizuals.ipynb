import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import time

# --- Section 1: Conceptual Comparison Tables (with Pandas) ---
# These tables are great for the "Methodology" or "Introduction" section
# to explain the tools you are analyzing.

def create_feature_comparison_table():
    """
    Creates a table comparing the different API functionalities.
    """
    data = {
        'Feature': [
            'Standard Text Generation', 
            'Grounded Text Generation', 
            'Image Generation'
        ],
        'API Endpoint / Model': [
            'gemini-2.5-flash-preview-09-2025', 
            'gemini-2.5-flash-preview-09-2025', 
            'imagen-3.0-generate-002'
        ],
        'Key API Property': [
            '`contents`', 
            '`contents` + `tools` (Google Search)', 
            '`instances` (with prompt)'
        ],
        'Primary Use Case': [
            'Creative writing, summarization, Q&A', 
            'Fact-based Q&A, real-time info', 
            'Generating novel images from text'
        ],
        'Output': [
            'Text', 
            'Text + Citations', 
            'Image (base64 encoded)'
        ]
    }
    df = pd.DataFrame(data)
    print("--- Table 1: API Feature Comparison ---")
    print(df.to_markdown(index=False))
    print("\n")
    return df

def create_grounding_quality_table():
    """
    Creates a qualitative table to show the *value* of grounding.
    You would fill this with real responses from your experiments.
    """
    data = {
        'Query': [
            "What was the top tech news story yesterday?",
            "Summarize the latest quarterly earnings for [Company X]."
        ],
        'Standard Response (No Grounding)': [
            "As an AI, I do not have access to real-time information...",
            "I do not have access to specific financial data after my last update..."
        ],
        'Grounded Response (With Google Search)': [
            "[Actual summary of the news story with citations.]",
            "[Specific summary of the earnings report with citations.]"
        ]
    }
    df = pd.DataFrame(data)
    print("--- Table 2: Qualitative Analysis of Grounding ---")
    print(df.to_markdown(index=False))
    print("\n")
    return df

# --- Section 2: Quantitative Performance Plots (with Matplotlib) ---
# These plots are essential for your "Results" or "Evaluation" section.

def plot_latency_distribution(mock_data=True):
    """
    Generates a box plot comparing the latency (response time)
    of different API tasks.
    
    In your real paper: You must replace the MOCK_DATA with real
    measurements from your API calls.
    """
    print("--- Plot 1: API Latency Distribution (Box Plot) ---")
    
    if mock_data:
        # Generate MOCK DATA
        # (Simulating that standard text is fastest, grounding adds overhead,
        # and image generation is the slowest)
        np.random.seed(42)
        latencies_standard = np.random.normal(loc=0.8, scale=0.2, size=50)
        latencies_grounded = np.random.normal(loc=2.5, scale=0.5, size=50)
        latencies_image = np.random.normal(loc=5.0, scale=1.0, size=50)
        
        # Ensure no negative latencies
        latencies_standard = np.abs(latencies_standard)
        latencies_grounded = np.abs(latencies_grounded)
        latencies_image = np.abs(latencies_image)
    else:
        # **HOW TO GET REAL DATA (Example):**
        # latencies_standard = []
        # for _ in range(50):
        #     start_time = time.time()
        #     # --- YOUR API CALL FOR STANDARD TEXT ---
        #     end_time = time.time()
        #     latencies_standard.append(end_time - start_time)
        # (Repeat for grounded and image calls)
        print("Using placeholder data. Replace with your measurements.")
        latencies_standard, latencies_grounded, latencies_image = [], [], []

    data_to_plot = [latencies_standard, latencies_grounded, latencies_image]

    fig, ax = plt.subplots()
    ax.set_title('API Task Latency Distribution')
    ax.set_ylabel('Response Time (seconds)')
    
    bp = ax.boxplot(data_to_plot, labels=['Standard Text', 'Grounded Text', 'Image Gen'], patch_artist=True)
    
    # Add colors for clarity
    colors = ['lightblue', 'lightgreen', 'lightpink']
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        
    ax.grid(True, linestyle='--', alpha=0.6)
    
    # Save the figure for your paper
    plt.savefig("api_latency_boxplot.png", dpi=300, bbox_inches='tight')
    print("Saved plot as 'api_latency_boxplot.png'")
    plt.show()
    print("\n")


def plot_scaling_performance(mock_data=True):
    """
    Generates scatter plots to show how performance scales with
    the size of the input prompt.
    
    In your real paper: You would run tests with prompts of
    different lengths and record the latency and output size.
    """
    print("--- Plot 2: Performance Scaling (Scatter Plots) ---")
    
    if mock_data:
        # Generate MOCK DATA
        prompt_lengths_chars = np.array([50, 150, 300, 500, 800, 1200])
        
        # Simulate latency increasing with prompt length (with some noise)
        base_latency = 0.5
        latency_noise = np.random.normal(0, 0.2, prompt_lengths_chars.shape)
        latencies = base_latency + (prompt_lengths_chars / 1000.0) + latency_noise
        
        # Simulate output length increasing with prompt length (with noise)
        base_output = 200
        output_noise = np.random.normal(0, 50, prompt_lengths_chars.shape)
        output_lengths_chars = base_output + (prompt_lengths_chars * 1.5) + output_noise
    else:
        # **HOW TO GET REAL DATA:**
        # prompt_lengths_chars = [50, 150, 300, 500, 800, 1200]
        # latencies = []
        # output_lengths_chars = []
        # for L in prompt_lengths_chars:
        #     my_prompt = "a " * (L // 2) # Create a prompt of roughly L chars
        #     start_time = time.time()
        #     # --- YOUR API CALL with my_prompt ---
        #     # response_text = ...
        #     end_time = time.time()
        #     latencies.append(end_time - start_time)
        #     output_lengths_chars.append(len(response_text))
        print("Using placeholder data. Replace with your measurements.")
        prompt_lengths_chars, latencies, output_lengths_chars = [], [], []

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # Plot 1: Prompt Length vs. Latency
    ax1.scatter(prompt_lengths_chars, latencies, c='blue', alpha=0.7, edgecolors='w')
    ax1.set_title('Prompt Length vs. API Latency')
    ax1.set_xlabel('Input Prompt Length (characters)')
    ax1.set_ylabel('Response Time (seconds)')
    ax1.grid(True, linestyle='--', alpha=0.6)
    
    # Optional: Add a trendline
    if mock_data:
        z = np.polyfit(prompt_lengths_chars, latencies, 1)
        p = np.poly1d(z)
        ax1.plot(prompt_lengths_chars, p(prompt_lengths_chars), "r--", label="Trend")
        ax1.legend()

    # Plot 2: Prompt Length vs. Output Length
    ax2.scatter(prompt_lengths_chars, output_lengths_chars, c='green', alpha=0.7, edgecolors='w')
    ax2.set_title('Prompt Length vs. Output Length')
    ax2.set_xlabel('Input Prompt Length (characters)')
    ax2.set_ylabel('Output Response Length (characters)')
    ax2.grid(True, linestyle='--', alpha=0.6)
    
    # Optional: Add a trendline
    if mock_data:
        z = np.polyfit(prompt_lengths_chars, output_lengths_chars, 1)
        p = np.poly1d(z)
        ax2.plot(prompt_lengths_chars, p(prompt_lengths_chars), "r--", label="Trend")
        ax2.legend()

    fig.suptitle('API Performance Scaling Analysis', fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    
    # Save the figure for your paper
    plt.savefig("api_scaling_scatter.png", dpi=300, bbox_inches='tight')
    print("Saved plot as 'api_scaling_scatter.png'")
    plt.show()

# --- Main execution ---
if __name__ == "__main__":
    print("Generating visualizations for IEEE paper...\n")
    
    # 1. Create and print tables
    create_feature_comparison_table()
    create_grounding_quality_table()
    
    # 2. Generate and show plots
    # Note: These will open in a new window or display inline in Colab/Jupyter
    plot_latency_distribution(mock_data=True)
    plot_scaling_performance(mock_data=True)
    
    print("\nGeneration complete. Remember to replace MOCK DATA with your real experimental results!")
    print("To get real latency: use `start = time.time()` before your API call and `latency = time.time() - start` after.")